# LLM Text Detection Project

## Overview

This project aims to develop machine learning models capable of accurately detecting whether a given essay was written by a student or generated by a large language model (LLM). The task involves implementing two types of models: one using deep learning methods and another using non-neural network approaches. The models will be evaluated based on the ROC-AUC score to ensure fair and competitive performance between both approaches.
Technical report is available at [[Report](https://github.com/JD-CEO/SBU_AdvancedML_FinalProject/blob/main/LLM%20Text%20Detection/LLM%20Text%20Detection%20Report.pdf)]

## Dataset Description

The dataset used in this competition consists of a mixture of student-written essays and essays generated by various LLMs. The data for the competition can be accessed via Kaggle:

- **Kaggle Link**: [LLM Text Detection Competition](https://www.kaggle.com/competitions/llm-detect-ai-generated-text/overview)
  
Due to certain restrictions, the dataset has also been made available via Google Drive:

- **Google Drive Link**: [Download Dataset](https://drive.google.com/file/d/1Mgz5tZ-T0YBzgI8jB61JuRNscngvMy6n/view?usp=sharing)

### Dataset Structure

The dataset contains three columns:
1. **Prompt_ID**: Identifies the unique writing prompt associated with each essay.
2. **Text**: Contains the essay content, which serves as the input for the machine learning models.
3. **Label**: The binary target variable for classification (0 for student-written, 1 for LLM-generated).

### Data Challenges

The dataset is highly imbalanced, with very few examples of LLM-generated essays (label = 1). Therefore, data balancing and augmentation techniques were employed to mitigate this issue and ensure more effective model training.

## Project Tasks

### Task Description

1. **Model Development**:
   - Implement at least two models:
     - One based on **deep learning methods** (e.g., LSTM, autoencoders).
     - One based on **non-neural network methods** (e.g., logistic regression, random forest).
   - Both models must be well-optimized and achieve a reasonable accuracy. Dismissing the non-neural network method due to low performance is not acceptable.

2. **Evaluation Metric**:
   - The performance of each model will be evaluated using the **ROC-AUC** metric, a robust measure for binary classification tasks.

## Methodology

### Exploratory Data Analysis (EDA)
- Conducted EDA to explore the datasetâ€™s structure and address the imbalance.
- Performed data cleaning, feature extraction, and augmentation to prepare the data for model training.

### Deep Learning Approach
- **Initial Model**: A neural network model was implemented using an autoencoder followed by a classifier. However, the initial attempts yielded limited success.
- **Advanced Model**: A Long Short-Term Memory (LSTM) model was developed to capture sequential patterns in the essay texts. While resource constraints limited detailed experimentation, the model demonstrated potential for handling the task.

### Non-Neural Network Approach
- **Logistic Regression**: Achieved strong performance with an ROC-AUC score of 0.9971.
- **Random Forest**: Showcased excellent results with an ROC-AUC score of 0.9999.
- **Gradient Boosting**: Also performed well with an ROC-AUC score of 0.9868, proving that classical machine learning models can be effective in this task.

## Datasets Used

Due to the scarcity of LLM-generated data, additional datasets were aggregated and utilized to balance the training data. These datasets provide a diverse range of essays generated by different LLMs, as well as human-written essays:

1. **Original Dataset**: 1,378 records from the Kaggle competition.
2. **Augmented Datasets**:
   - Several external datasets were introduced, including ones generated by LLMs such as GPT, Claude, and PaLM, as well as a variety of student essays.
   - A total of 527,720 unique records were aggregated from multiple sources, enhancing the dataset for training purposes.

### Key Datasets:
- **daigt-v2-train-dataset**: 44,868 records
- **daigt-external-dataset**: 4,842 records
- **falcon-dataset**: 7,000 records generated by high-capacity LLMs
- **mixtral and palm-datasets**: 3,865 and 1,384 records, respectively, generated by specific LLM techniques
- **augmented-data-dataset**: 433,564 records augmenting the training set

Each dataset contributes to building a robust model by enhancing the diversity of training examples and balancing the class distribution.

## Conclusion

The project successfully demonstrated that detecting LLM-generated text can be approached with both deep learning and classical machine learning techniques. Our results underscore the capability of traditional models, such as logistic regression and random forests, in achieving high classification performance on the task.

While further refinement of deep learning models like LSTM is possible, the fairness and effectiveness of both approaches were maintained as required by the competition guidelines.

## Contributors
- **Mehrdad Baradaran**
- **Hossein Yahyaei**
- **Katayoun Kobraei**
